[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stock Return Analysis",
    "section": "",
    "text": "1 Introduction\nUnderstanding the characteristics of financial data is a first step for quantitative research. For this project, we chose to analyze stock daily return data from the S&P 500 index because it offers a diverse and widely studied dataset that is crucial for exploring key financial metrics. By examining raw stock return, we can compute essential statistics such as mean and variance (volatility), correlation between stocks, and the distribution of daily returns. This analysis forms the foundation for understanding how individual stocks behave and interact within the broader market.\nAdditionally, the project involves implementing simple factor models like the Capital Asset Pricing Model (CAPM) and Fama-French models. Through linear regression, I will estimate factor loadings (betas) and excessive returns (alphas), evaluating their statistical properties such as mean, variance, and distribution. I will also test the performance of these linear models using t-statistics and p-values to understand how well the chosen factors explain stock returns linearly. While this project is exploratory and does not include advanced QR applications like portfolio optimization or alpha generation, it serves as a critical step in building the analytical and statistical tools required for such work."
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "2.1 Description",
    "text": "2.1 Description\nThe data for this project is sourced from Yahoo Finance, a widely used platform providing comprehensive historical market data for stocks and indices. The dataset includes daily records for multiple assets from the S&P 500 index, with columns such as Date, Open, High, Low, Close, Adjusted Close, and Volume. The primary focus is on the Close price, which will be used to calculate the daily log returns. Log returns are derived by taking the natural logarithm of the ratio of successive closing prices. These returns are central to the analysis as they standardize price changes, making the data scale-independent and additive over time, which simplifies statistical modeling and comparisons. The log return is preferred over simple return because it accounts for compounding effects and ensures symmetry between positive and negative percentage changes, making it ideal for financial data analysis.\n\n\nCode\n# Display the first few rows of the resulting dataframe\nhead(sp500_stock_df)\n\n\n        Date     Open     High      Low    Close    Volume Adjusted Ticker\n1 2013-01-02 19.77929 19.82143 19.34393 19.60821 560518000 16.68734   AAPL\n2 2013-01-03 19.56714 19.63107 19.32143 19.36071 352965200 16.47671   AAPL\n3 2013-01-04 19.17750 19.23679 18.77964 18.82143 594333600 16.01776   AAPL\n4 2013-01-07 18.64286 18.90357 18.40000 18.71071 484156400 15.92354   AAPL\n5 2013-01-08 18.90036 18.99607 18.61607 18.76107 458707200 15.96639   AAPL\n6 2013-01-09 18.66071 18.75036 18.42821 18.46786 407604400 15.71686   AAPL\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Define the tickers of interest\nselected_tickers &lt;- c(\"AAPL\", \"TSLA\", \"NVDA\", \"JNJ\", \"JPM\", \"BRK-B\")\n\n# Filter the dataframe for the selected tickers\nsp500_subset &lt;- sp500_stock_df %&gt;%\n  filter(Ticker %in% selected_tickers)\n\n# Create the line plot\nggplot(sp500_subset, aes(x = Date, y = Adjusted, color = Ticker)) +\n  geom_line() +\n  labs(\n    title = \"Adjusted Close Prices Over Time\",\n    x = \"Date\",\n    y = \"Adjusted Close\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nAlthough an initial glance at raw price data may provide a sense of how various assets evolve over time, such plots are not ideal for comparative analysis. Each asset operates within its own price range, making direct comparisons misleading. To enable a fair evaluation across different assets, we employ measures such as daily returns and cumulative returns. A commonly used definition for the daily return is:\n\\[\n\\text{Daily Return} = \\frac{P_{t} - P_{t-1}}{P_{t-1}}\n\\]\n\n\nCode\ndaily_returns &lt;- function(data) {\n  # Ensure the data is sorted by Date\n  data &lt;- data[order(data$Date), ]\n  \n  # Calculate daily returns using Adjusted prices\n  # diff(data$Adjusted) gives P_t - P_{t-1}, and head(data$Adjusted, -1) gives P_{t-1}\n  returns &lt;- diff(data$Adjusted) / head(data$Adjusted, -1)\n  \n  # Create a new data frame with Date and Return\n  result &lt;- data.frame(\n    Date = data$Date[-1],    # Remove the first date since there's no previous day return\n    Return = returns\n  )\n  \n  return(result)\n}\n\n\n\n\nCode\nAAPL_price &lt;- subset(sp500_stock_df, Ticker == \"AAPL\")\nAAPL_return &lt;- daily_returns((AAPL_price))\nJNJ_price &lt;- subset(sp500_stock_df, Ticker == \"JNJ\")\nJNJ_return &lt;- daily_returns((JNJ_price))\n\n\n\n2.1.1 Problems with visualizing return over time, and the histogram of return\n\n\nCode\nlibrary(patchwork)\n\n# Create AAPL daily returns plot\np1 &lt;- ggplot(AAPL_return, aes(x = Date, y = Return)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"AAPL Daily Returns\",\n    x = \"Date\",\n    y = \"Daily Return\"\n  ) +\n  theme_minimal()\n\n# Create JNJ daily returns plot\np2 &lt;- ggplot(JNJ_return, aes(x = Date, y = Return)) +\n  geom_line(color = \"red\") +\n  labs(\n    title = \"JNJ Daily Returns\",\n    x = \"Date\",\n    y = \"Daily Return\"\n  ) +\n  theme_minimal()\n\n# Display side-by-side\np1 + p2\n\n\n\n\n\n\n\nCode\nmean_aapl &lt;- mean(AAPL_return$Return, na.rm = TRUE)\nsd_aapl &lt;- sd(AAPL_return$Return, na.rm = TRUE)\n\n# Plot for AAPL\np1 &lt;- ggplot(AAPL_return, aes(x = Return)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 0.01, \n                 fill = \"steelblue\", \n                 color = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean_aapl, sd = sd_aapl), \n                color = \"red\", \n                size = 1) +\n  labs(\n    title = \"AAPL Daily Returns Distribution\",\n    x = \"Daily Return\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\n# Calculate mean and sd for JNJ returns\nmean_jnj &lt;- mean(JNJ_return$Return, na.rm = TRUE)\nsd_jnj &lt;- sd(JNJ_return$Return, na.rm = TRUE)\n\n# Plot for JNJ\np2 &lt;- ggplot(JNJ_return, aes(x = Return)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 0.01, \n                 fill = \"steelblue\", \n                 color = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean_jnj, sd = sd_jnj), \n                color = \"red\", \n                size = 1) +\n  labs(\n    title = \"JNJ Daily Returns Distribution\",\n    x = \"Daily Return\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n# Display both plots side-by-side using patchwork\np1 + p2\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nWill investments on AAPL and JNJ perform similarly over time?\n\n\n2.1.2 But when it compounds…\n\n\nCode\nAAPL_compounding_return &lt;- AAPL_return %&gt;%\n  mutate(Compounding_Return = cumprod(1 + Return)) %&gt;%\n  mutate(Ticker = \"AAPL\")\n\n# Compute compounding returns for JNJ\nJNJ_compounding_return &lt;- JNJ_return %&gt;%\n  mutate(Compounding_Return = cumprod(1 + Return)) %&gt;%\n  mutate(Ticker = \"JNJ\")\n\n# Combine the two\ncombined_compounding_return &lt;- bind_rows(AAPL_compounding_return, JNJ_compounding_return)\n\n# Plot both AAPL and JNJ on the same plot\nggplot(combined_compounding_return, aes(x = Date, y = Compounding_Return, color = Ticker)) +\n  geom_line() +\n  labs(\n    title = \"AAPL and JNJ Compounding Returns Over Time\",\n    x = \"Date\",\n    y = \"Compounding Return\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "data.html#the-limitations-of-simple-plots-for-analyzing-returns",
    "href": "data.html#the-limitations-of-simple-plots-for-analyzing-returns",
    "title": "2  Data",
    "section": "2.2 The Limitations of Simple Plots for Analyzing Returns",
    "text": "2.2 The Limitations of Simple Plots for Analyzing Returns\nThe daily return plot looks like noise, oscillating about 0, and we cannot make anything out of it. The histogram of returns has a mean close to 0 and resembles a normal distribution. However, when we look at the compounding return over time, it clearly goes up and is no longer messy. Why is it so hard to capture information out of the return? It’s due to market efficiency: if the returns were obviously informative, then something would be wrong, because no one in the market is foolish. If the return data showed anything other than noise, it would mean easy profit opportunities, which would quickly vanish as participants exploit them through arbitrage. Unless you engage in high-frequency trading, it’s impossible to take advantage of such fleeting signals. Thus, it is perfectly normal for the returns to look like noise, as it reflects an efficient market."
  },
  {
    "objectID": "data.html#is-return-informative",
    "href": "data.html#is-return-informative",
    "title": "2  Data",
    "section": "2.3 Is return informative?",
    "text": "2.3 Is return informative?\nBut why does the cumulative return plot appear clearer? In the long term, there are still profits to be gained, influenced by industry trends or other large-scale factors. While these factors are hidden within the daily noise, they become evident when returns accumulate. Simply using basic plots like this, however, won’t help us uncover these subtleties. There are many important patterns hidden in the return data, that are not easy to visualize. We need more complex models to make the non visible pattern visible.\n\n\nCode\naapl_stats &lt;- AAPL_return %&gt;%\n  summarize(\n    mean = mean(Return, na.rm = TRUE),\n    q25 = quantile(Return, 0.25, na.rm = TRUE),\n    q50 = quantile(Return, 0.5, na.rm = TRUE),\n    q75 = quantile(Return, 0.75, na.rm = TRUE),\n    sd = sd(Return, na.rm = TRUE)\n  )\n\n# Calculate stats for JNJ\njnj_stats &lt;- JNJ_return %&gt;%\n  summarize(\n    mean = mean(Return, na.rm = TRUE),\n    q25 = quantile(Return, 0.25, na.rm = TRUE),\n    q50 = quantile(Return, 0.5, na.rm = TRUE),\n    q75 = quantile(Return, 0.75, na.rm = TRUE),\n    sd = sd(Return, na.rm = TRUE)\n  )\n\n# Print the stats\ncat(\"AAPL Statistics:\\n\")\n\n\nAAPL Statistics:\n\n\nCode\nprint(aapl_stats)\n\n\n         mean         q25          q50        q75         sd\n1 0.001042554 -0.00738776 0.0008967392 0.01024167 0.01787848\n\n\nCode\ncat(\"\\nJNJ Statistics:\\n\")\n\n\n\nJNJ Statistics:\n\n\nCode\nprint(jnj_stats)\n\n\n          mean         q25          q50         q75         sd\n1 0.0004576925 -0.00463619 0.0004374614 0.005997656 0.01107514\n\n\nCode\n# Combine the data for plotting\ncombined_returns &lt;- rbind(\n  data.frame(Ticker = \"AAPL\", Return = AAPL_return$Return),\n  data.frame(Ticker = \"JNJ\", Return = JNJ_return$Return)\n)\n\n# Boxplot comparison\nggplot(combined_returns, aes(x = Ticker, y = Return, fill = Ticker)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of AAPL and JNJ Daily Returns\",\n    x = \"Ticker\",\n    y = \"Daily Return\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD We can still extract some useful information from these results:\n\nAAPL return indeed has a higher mean than JNJ return.\nFrom the boxplot, we can see AAPL is more volatile than JNJ.\n(Is that essentially a good thing?)\n\n\n\nCode\nwrite.csv(sp500_stock_df, \"sp500_stock_df.csv\", row.names = FALSE)\n\n\n======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; add390be3b356c7fed06a29e21fa0c7c2deb8b81"
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.4 Missing value analysis",
    "text": "2.4 Missing value analysis"
  },
  {
    "objectID": "results.html#the-fama-french-factor-model",
    "href": "results.html#the-fama-french-factor-model",
    "title": "3  Results",
    "section": "3.1 The Fama-French Factor Model",
    "text": "3.1 The Fama-French Factor Model\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tibble)\nsp500_stock_df &lt;- read.csv(\"sp500_stock_df.csv\")\nsp500_stock_df$Date &lt;- as.Date(sp500_stock_df$Date)\nff_df &lt;- read.csv(\"F-F_Research_Data_Factors_daily.csv\", skip = 4, header = TRUE)\n\n\n\n\nCode\nff_df &lt;- ff_df %&gt;% mutate(across(where(is.numeric), ~ . / 100))\n\n\n\n\nCode\nff_df$X &lt;- as.Date(ff_df$X, format = \"%Y%m%d\")\ncolnames(ff_df)[colnames(ff_df) == \"X\"] &lt;- \"Date\"\n\n\n\n\nCode\nlibrary(tidyr)\nadjusted_close &lt;- sp500_stock_df %&gt;%\n  select(Date, Adjusted, Ticker) %&gt;%  # Keep only necessary columns\n  pivot_wider(names_from = Ticker, values_from = Adjusted)\n\nreturns &lt;- adjusted_close %&gt;%\n  mutate(across(where(is.numeric), ~ c(NA, diff(.) / lag(.)[-1]))) %&gt;%\n  filter(complete.cases(.))\n\n\n\n\nCode\nexcess_returns &lt;- returns %&gt;%\n  left_join(ff_df, by = \"Date\") %&gt;%  # Merge by Date\n  mutate(across(\n    .cols = setdiff(names(.), c(\"Date\", \"RF\")),  # Select all columns except \"Date\" and \"RF\"\n    .fns = ~ . - RF\n  )) %&gt;%\n  select(-Mkt.RF, -SMB, -HML, -RF)\ntail(excess_returns)\n\n\n# A tibble: 6 × 21\n  Date            AAPL        MSFT     GOOGL     AMZN     TSLA     NVDA  `BRK-B`\n  &lt;date&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-12-21 -0.000980  0.00767      1.48e-2  1.11e-2  0.0296   0.0181   2.01e-3\n2 2023-12-22 -0.00576   0.00257      7.41e-3 -2.94e-3 -0.00791 -0.00348  7.17e-4\n3 2023-12-26 -0.00305   0.00000357   2.03e-6 -2.75e-4  0.0159   0.00899  8.00e-4\n4 2023-12-27  0.000308 -0.00178     -8.34e-3 -6.66e-4  0.0186   0.00259  1.26e-4\n5 2023-12-28  0.00202   0.00302     -1.21e-3  5.09e-5 -0.0318   0.00191  1.53e-3\n6 2023-12-29 -0.00563   0.00182     -4.06e-3 -9.60e-3 -0.0188  -0.00021 -2.75e-3\n# ℹ 13 more variables: UNH &lt;dbl&gt;, JNJ &lt;dbl&gt;, V &lt;dbl&gt;, PG &lt;dbl&gt;, JPM &lt;dbl&gt;,\n#   XOM &lt;dbl&gt;, BAC &lt;dbl&gt;, PFE &lt;dbl&gt;, HD &lt;dbl&gt;, MA &lt;dbl&gt;, KO &lt;dbl&gt;, DIS &lt;dbl&gt;,\n#   NFLX &lt;dbl&gt;\n\n\n\n\nCode\nff3 &lt;- ff_df %&gt;%\n  select(-RF) %&gt;% \n  filter(Date %in% excess_returns$Date)  \n\ntail(ff3)\n\n\n           Date  Mkt.RF     SMB     HML\n2762 2023-12-21  0.0109  0.0073 -0.0002\n2763 2023-12-22  0.0020  0.0064  0.0010\n2764 2023-12-26  0.0048  0.0071  0.0043\n2765 2023-12-27  0.0016  0.0014  0.0010\n2766 2023-12-28 -0.0001 -0.0036  0.0002\n2767 2023-12-29 -0.0043 -0.0114 -0.0036\n\n\n\n\nCode\nexcess_returns_df &lt;- as.data.frame(excess_returns)\n# Convert Date column to proper Date type if needed\nexcess_returns_df$Date &lt;- as.Date(excess_returns_df$Date)\n# Set the Date column as row names\nrownames(excess_returns_df) &lt;- as.character(excess_returns_df$Date)\n# Remove the Date column\nexcess_returns_df$Date &lt;- NULL\n\nrownames(ff3) &lt;- ff3$Date\nff3$Date &lt;- NULL\n\n\n\n\nCode\nrolling_ff3_coefficients &lt;- function(returns, ff3_factors, window_size = 252, check_interval = 21) {\n  # Check that returns and ff3_factors have the same number of rows\n  if (nrow(returns) != nrow(ff3_factors)) {\n    stop(\"returns and ff3_factors must have the same number of rows.\")\n  }\n  \n  n &lt;- nrow(returns)\n  stock_names &lt;- colnames(returns)\n  factor_names &lt;- colnames(ff3_factors)\n  \n  # Initialize lists to store rolling results\n  rolling_alpha &lt;- list()\n  rolling_betas &lt;- vector(\"list\", 3) # For 3 factors\n  names(rolling_betas) &lt;- paste0(\"beta_\", 1:3)\n  \n  # Prepare empty slots for each stock\n  for (stock in stock_names) {\n    rolling_alpha[[stock]] &lt;- numeric(0)\n    for (beta_key in names(rolling_betas)) {\n      rolling_betas[[beta_key]][[stock]] &lt;- numeric(0)\n    }\n  }\n  \n  rolling_dates &lt;- character(0)\n  \n  # Iterate over rolling windows\n  for (i in seq(window_size, n, by = check_interval)) {\n    start_idx &lt;- i - window_size + 1\n    end_idx &lt;- i\n    \n    returns_window &lt;- returns[start_idx:end_idx, , drop = FALSE]\n    factors_window &lt;- ff3_factors[start_idx:end_idx, , drop = FALSE]\n    \n    # Construct the model formula: y ~ MKT_RF + SMB + HML\n    model_formula &lt;- as.formula(paste(\"y ~\", paste(factor_names, collapse = \" + \")))\n    \n    for (stock in stock_names) {\n      y &lt;- returns_window[[stock]]\n      data_for_reg &lt;- cbind(y = y, factors_window)\n      \n      # Fit the linear model\n      model &lt;- lm(model_formula, data = data_for_reg)\n      coefs &lt;- coef(model)\n      \n      # Store alpha and betas\n      rolling_alpha[[stock]] &lt;- c(rolling_alpha[[stock]], coefs[1]) # alpha (intercept)\n      for (j in 1:3) {\n        rolling_betas[[paste0(\"beta_\", j)]][[stock]] &lt;- c(rolling_betas[[paste0(\"beta_\", j)]][[stock]], coefs[j+1])\n      }\n    }\n    \n    # Record the date of the last observation in the window\n    rolling_dates &lt;- c(rolling_dates, rownames(returns)[end_idx])\n  }\n  \n  # Convert lists to data frames\n  df_alpha &lt;- as.data.frame(rolling_alpha, row.names = rolling_dates)\n  df_betas &lt;- lapply(rolling_betas, function(x) as.data.frame(x, row.names = rolling_dates))\n  \n  # Return a list with alpha and each beta DataFrame\n  return(list(\n    alpha = df_alpha,\n    beta_1 = df_betas[[\"beta_1\"]],\n    beta_2 = df_betas[[\"beta_2\"]],\n    beta_3 = df_betas[[\"beta_3\"]]\n  ))\n}\n\n# Example usage:\n# result &lt;- rolling_ff3_coefficients(returns, ff3_factors)\n# df_alpha &lt;- result$alpha\n# df_beta_1 &lt;- result$beta_1\n# df_beta_2 &lt;- result$beta_2\n# df_beta_3 &lt;- result$beta_3\n\n\nWe apply the Fama-French three-factor model to the returns of the selected assets. The model is expressed as:\n\\[\nR_i - R_f = \\alpha + \\beta_1 (R_m - R_f) + \\beta_2 \\text{SMB} + \\beta_3 \\text{HML} + \\epsilon\n\\]\nWhere: - \\(R_i\\): Return of the asset.\n\n\\(R_f\\): Risk-free rate.\n\\(R_m\\): Market return.\n\\(\\text{SMB}\\): Small-minus-Big factor, representing the size effect.\n\\(\\text{HML}\\): High-minus-Low factor, representing the value effect.\n\\(\\alpha\\): Intercept (alpha).\n$ _1, _2, _3$: Factor loadings.\n\\(\\epsilon\\): Residual error term.\n\nInstead of focusing solely on returns, our analysis emphasizes the \\(\\alpha\\) and the factor loadings \\(\\beta_1, \\beta_2, \\beta_3\\).\n\n\nCode\nfactor_loadings &lt;- rolling_ff3_coefficients(excess_returns_df, ff3, window_size = 630)\nhead(factor_loadings$alpha)\n\n\n                   AAPL         MSFT         GOOGL         AMZN        TSLA\n2015-07-06 0.0003826976 0.0002519413 -2.165869e-05 9.483022e-05 0.002817870\n2015-08-04 0.0004840270 0.0003558080  1.676533e-04 3.980341e-04 0.002621231\n2015-09-02 0.0006314758 0.0003268785  1.769927e-04 4.595927e-04 0.002777581\n2015-10-02 0.0006410364 0.0004157632  3.286810e-04 6.760671e-04 0.002657863\n2015-11-02 0.0006770733 0.0003329106  3.678968e-04 8.565453e-04 0.001954208\n2015-12-02 0.0006001235 0.0003526157  4.095854e-04 9.316304e-04 0.001075275\n                   NVDA         BRK.B          UNH           JNJ            V\n2015-07-06 0.0001856673  1.328795e-05 0.0007720469  6.715991e-05 0.0002428375\n2015-08-04 0.0003026348  5.168182e-05 0.0007608343  1.914612e-05 0.0004032871\n2015-09-02 0.0005109694 -1.606046e-06 0.0008684762 -1.744110e-05 0.0004271619\n2015-10-02 0.0007929165 -4.157491e-05 0.0007006565 -6.945329e-05 0.0004233442\n2015-11-02 0.0007645336 -4.524336e-05 0.0007277140 -5.064713e-05 0.0003440647\n2015-12-02 0.0009540849 -1.040841e-04 0.0006561300  2.580040e-06 0.0004569077\n                      PG          JPM           XOM           BAC           PFE\n2015-07-06 -6.683648e-05 1.136685e-04 -0.0004996520 -1.651476e-05 -7.941138e-06\n2015-08-04 -2.954303e-04 1.592889e-04 -0.0005621190  2.593854e-04 -4.814928e-06\n2015-09-02 -3.430701e-04 8.583783e-05 -0.0005400569  1.836377e-04 -7.614855e-05\n2015-10-02 -2.970566e-04 1.223812e-04 -0.0004791434  9.311710e-05 -8.243929e-05\n2015-11-02 -2.351699e-04 1.608895e-04 -0.0003553496  1.449749e-04 -3.633357e-05\n2015-12-02 -2.068361e-04 7.745179e-05 -0.0004038340  1.508718e-04 -3.673794e-05\n                     HD           MA            KO          DIS        NFLX\n2015-07-06 0.0003710481 0.0002275181 -2.038845e-04 0.0006551624 0.002806982\n2015-08-04 0.0003752316 0.0002845652 -1.087278e-04 0.0006584919 0.002203376\n2015-09-02 0.0004489648 0.0003348453 -1.463281e-04 0.0005117148 0.001911452\n2015-10-02 0.0004842864 0.0003847118 -1.471616e-04 0.0005309267 0.002164452\n2015-11-02 0.0004484185 0.0003808372 -1.658065e-04 0.0004682531 0.001697815\n2015-12-02 0.0005224237 0.0003439808 -6.908144e-05 0.0005005945 0.001887403\n\n\n\n\nCode\nlibrary(ggplot2)\n\nalphas_df &lt;- factor_loadings$alpha %&gt;%\n  rownames_to_column(var = \"Date\")\n \nbeta1_df &lt;- factor_loadings$beta_1 %&gt;%\n  rownames_to_column(var = \"Date\")\n\nbeta2_df &lt;- factor_loadings$beta_2 %&gt;%\n  rownames_to_column(var = \"Date\")\n\nbeta3_df &lt;- factor_loadings$beta_3 %&gt;%\n  rownames_to_column(var = \"Date\")\n\n\n\n3.1.1 Observations on Alpha Distributions\nAlpha measures an investment’s excess return over a benchmark – the market or factor-based expectations. In models like Fama-French, it captures the portion of returns unexplained by factors such as market risk or size. In reality, alpha often reverts to zero as market inefficiencies are corrected, making it hard to find and sustain a constantly positive alpha. Investors aim for consistent, positive alpha as a source of stable excess return.\n\n\nCode\nlibrary(ggridges)\nalphas_long &lt;- alphas_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"Alpha\")\n\nggplot(alphas_long, aes(x = as.Date(Date), y = Alpha, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Alphas Over Time\",\n    x = \"Date\",\n    y = \"Alpha\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nalphas_long &lt;- alphas_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanAlpha = mean(Alpha)) %&gt;%\n  ungroup()\n\nggplot(alphas_long, aes(x = Alpha, y = reorder(Stock, MeanAlpha), fill = MeanAlpha)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Alpha\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Alpha Distributions by Stock\",\n    x = \"Alpha\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.000137\n\n\n\n\n\nMost alphas fluctuate around zero.\n\n3.1.1.1 Stocks with no excess return: PG, BRK.B, JNJ, KO, PFE\n\nThe alpha distributions for these low-performing stocks exhibit compact and peaked shapes, with the mean centered around zero.\nThis indicates that these stocks consistently achieve alpha values near zero, aligning closely with the efficient market hypothesis. In an efficient market, these stocks do not present persistent opportunities for abnormal returns.\n\n\n\n3.1.1.2 Stocks with stable positive excess return: AAPL, MSFT, HD, GOOGL, JPM, V\n\nThese high-performing stocks have a consistently positive alpha, the distribution is less compact, but still has a peak.\nThis indicates that these stocks consistently outperform the Fama-French Factor Model, with a constant excess return. The excess return is unexplained by Market Return, SMB and HML. We might be able to explain the excess return by introducing new factors.\nLong-term investment on these stocks will yield a positive excess return over Fama-French Factors.\n\n\n\n3.1.1.3 Stocks with very unstable positive excess return: NVDA, TSLA, NFLX, AMZN\n\nThe alpha distributions for high-performing stocks display very fat tails and lack a distinct peak, resulting in a flat and widespread shape.\nThese stocks exhibit significantly higher mean alphas, indicating periods of consistent outperformance of the efficient market. However, the flat distribution curve suggests that their alpha is highly unstable, and does not stabilize around a single value for long. Once the excess return appear, it will soon be exploited by market players, but the excess return often reappear shortly.\nThese stocks are uprising assets with great potentials and risks at the same time. The profit could be very large over a short period of time, but it is unpredictable.\n\n\n\n\n3.1.2 Observations on the Distributions of exposure to Market Risk\n\\(\\beta_1\\) in the Fama-French model represents the factor loading for market risk, measuring how sensitive an asset is to overall market movements. A \\(\\beta_1 &gt; 1\\) indicates that the asset is more volatile than the market, while \\(\\beta_1 &lt; 1\\) suggests it is less sensitive to market changes. It evaluates the portion of an asset’s returns driven by systematic market risk, making it crucial for understanding exposure to broad economic trends. Investors rely on \\(\\beta_1\\) to assess the asset’s alignment with market performance and its role in portfolio risk management.\nAlthough it often appears to be so, a high \\(\\beta_1\\) does not essentially mean a good sign. Because high volatility also brings more risk, causing significant loss when the market is going down. when the market is going upward, it is usually preferred to invest on assets with higher exposure to market risk.\n\n\nCode\nbeta1_long &lt;- beta1_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"beta1\")\n\nggplot(beta1_long, aes(x = as.Date(Date), y = beta1, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Beta1 Over Time\",\n    x = \"Date\",\n    y = \"beta1\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nbeta1_long &lt;- beta1_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanBeta1 = mean(beta1)) %&gt;%\n  ungroup()\n\nggplot(beta1_long, aes(x = beta1, y = reorder(Stock, MeanBeta1), fill = MeanBeta1)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Beta1\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Beta1 Distributions by Stock\",\n    x = \"Beta\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.0364\n\n\n\n\n\n\n3.1.2.1 Stocks with high exposure to Market Risk: NVDA, TSLA, NFLX, MSFT, JPM, AMZN, AAPL, GOOGL, etc.\nStocks that have a high \\(\\beta_1\\), reflecting significant sensitivity to market movements. These stocks often belong to technology, growth, or finance sectors, which are more volatile and influenced by broad economic trends.\n\n\n3.1.2.2 Stocks with low exposure to Market Risk: BRK.B, UNH, XOM, JNJ, KO, PG, etc.\nStocks that have a low \\(\\beta_1\\), indicating less sensitivity to market movements. These companies are often in stable industries such as consumer staples, healthcare, or energy, providing consistent performance regardless of market volatility.\n\n\n\n3.1.3 Observations on the Distributions of exposure to SMB and HML\nDue to the length of the project, the effects of the \\(\\beta_2\\) and \\(\\beta_3\\) will not be discussed here. The plots of the factor loadings by stocks are shown below. We can notice that the coefficient of SMB and coefficient of HML seem to be negatively correlated. This is because small-cap stocks (high SMB) often exhibit growth characteristics (low HML), while large-cap stocks (low SMB) tend to align with value characteristics (high HML).\n\n\nCode\nbeta2_long &lt;- beta2_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"beta2\")\n\nggplot(beta2_long, aes(x = as.Date(Date), y = beta2, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Beta2 Over Time\",\n    x = \"Date\",\n    y = \"beta2\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nbeta2_long &lt;- beta2_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanBeta2 = mean(beta2)) %&gt;%\n  ungroup()\n\nggplot(beta2_long, aes(x = beta2, y = reorder(Stock, MeanBeta2), fill = MeanBeta2)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Beta2\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Beta2 Distributions by Stock\",\n    x = \"Beta2\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.0471\n\n\n\n\n\n\n\nCode\nbeta3_long &lt;- beta3_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"beta3\")\n\nggplot(beta3_long, aes(x = as.Date(Date), y = beta3, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Beta3 Over Time\",\n    x = \"Date\",\n    y = \"beta3\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nbeta3_long &lt;- beta3_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanBeta3 = mean(beta3)) %&gt;%\n  ungroup()\n\nggplot(beta3_long, aes(x = beta3, y = reorder(Stock, MeanBeta3), fill = MeanBeta3)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Beta3\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Beta3 Distributions by Stock\",\n    x = \"Beta3\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.0587\n\n\n\n\n\n\n\n3.1.4 Relationship between Factor Loadings\nAs we can see more clearly in the parallel coordinate plot, the seemingly positive relation between \\(\\alpha\\) and \\(\\beta_1\\) is not very strong. We can only say with confidence that \\(\\beta_2\\) and \\(\\beta_3\\) are indeed negatively correlated.\n\n\nCode\nalphas_median &lt;- apply(alphas_df[-1], 2, median)\nbeta1_median &lt;- apply(beta1_df[-1], 2, median)\nbeta2_median &lt;- apply(beta2_df[-1], 2, median)\nbeta3_median &lt;- apply(beta3_df[-1], 2, median)\n\nloading_df &lt;- data.frame(\n  stock = names(alphas_median),\n  alpha = alphas_median,\n  beta1 = beta1_median,\n  beta2 = beta2_median,\n  beta3 = beta3_median\n)\n\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\n# Create the parallel coordinate plot\nggparcoord(\n  data = loading_df,\n  columns = 2:5,       # Select columns for parallel coordinates (alpha, beta1, beta2, beta3)\n  groupColumn = 1,     # Use the stock column for grouping lines\n  scale = \"uniminmax\"  # Scale the data to [0, 1] for better visualization\n) +\n  labs(\n    title = \"Parallel Coordinate Plot for Stock Factor Loadings\",\n    x = \"Factors\",\n    y = \"Scaled Values\",\n    color = \"Stock\"\n  ) +\n  theme_minimal()"
  }
]