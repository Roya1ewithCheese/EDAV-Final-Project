[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stock Return Analysis",
    "section": "",
    "text": "1 Introduction\nUnderstanding the characteristics of financial data is a first step for quantitative research. For this project, we chose to analyze stock daily return data from the S&P 500 index because it offers a diverse and widely studied dataset that is crucial for exploring key financial metrics. By examining raw stock return, we can compute essential statistics such as mean and variance (volatility), correlation between stocks, and the distribution of daily returns. This analysis forms the foundation for understanding how individual stocks behave and interact within the broader market.\nAdditionally, the project involves implementing simple factor models like the Capital Asset Pricing Model (CAPM) and Fama-French models. Through linear regression, I will estimate factor loadings (betas) and excessive returns (alphas), evaluating their statistical properties such as mean, variance, and distribution. I will also test the performance of these linear models using t-statistics and p-values to understand how well the chosen factors explain stock returns linearly. While this project is exploratory and does not include advanced QR applications like portfolio optimization or alpha generation, it serves as a critical step in building the analytical and statistical tools required for such work."
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "2.1 Description",
    "text": "2.1 Description\nThe data for this project is sourced from Yahoo Finance, a widely used platform providing comprehensive historical market data for stocks and indices. The dataset includes daily records for multiple assets from the S&P 500 index, with columns such as Date, Open, High, Low, Close, Adjusted Close, and Volume. The primary focus is on the Close price, which will be used to calculate the daily log returns. Log returns are derived by taking the natural logarithm of the ratio of successive closing prices. These returns are central to the analysis as they standardize price changes, making the data scale-independent and additive over time, which simplifies statistical modeling and comparisons. The log return is preferred over simple return because it accounts for compounding effects and ensures symmetry between positive and negative percentage changes, making it ideal for financial data analysis.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Define the tickers of interest\nselected_tickers &lt;- c(\"AAPL\", \"TSLA\", \"NVDA\", \"JNJ\", \"JPM\", \"BRK-B\")\n\n# Filter the dataframe for the selected tickers\nsp500_subset &lt;- sp500_stock_df %&gt;%\n  filter(Ticker %in% selected_tickers)\n\n# Create the line plot\nggplot(sp500_subset, aes(x = Date, y = Adjusted, color = Ticker)) +\n  geom_line() +\n  labs(\n    title = \"Adjusted Close Prices Over Time\",\n    x = \"Date\",\n    y = \"Adjusted Close\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "data.html#return",
    "href": "data.html#return",
    "title": "2  Data",
    "section": "2.2 Return",
    "text": "2.2 Return\nAlthough an initial glance at raw price data may provide a sense of how various assets evolve over time, such plots are not ideal for comparative analysis. Each asset operates within its own price range, making direct comparisons misleading. To enable a fair evaluation across different assets, we employ measures such as daily returns and cumulative returns. A commonly used definition for the daily return is:\n\\[\n\\text{Daily Return} = \\frac{P_{t} - P_{t-1}}{P_{t-1}}\n\\]\n\n\nCode\ndaily_returns &lt;- function(data) {\n  # Ensure the data is sorted by Date\n  data &lt;- data[order(data$Date), ]\n  \n  # Calculate daily returns using Adjusted prices\n  # diff(data$Adjusted) gives P_t - P_{t-1}, and head(data$Adjusted, -1) gives P_{t-1}\n  returns &lt;- diff(data$Adjusted) / head(data$Adjusted, -1)\n  \n  # Create a new data frame with Date and Return\n  result &lt;- data.frame(\n    Date = data$Date[-1],    # Remove the first date since there's no previous day return\n    Return = returns\n  )\n  \n  return(result)\n}\n\nAAPL_price &lt;- subset(sp500_stock_df, Ticker == \"AAPL\")\nAAPL_return &lt;- daily_returns((AAPL_price))\nJNJ_price &lt;- subset(sp500_stock_df, Ticker == \"JNJ\")\nJNJ_return &lt;- daily_returns((JNJ_price))\n\n\n\n2.2.1 Problems with visualizing return over time, and the histogram of return\n\n\nCode\nlibrary(patchwork)\n\n\n\n# Create AAPL daily returns plot\np1 &lt;- ggplot(AAPL_return, aes(x = Date, y = Return)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"AAPL Daily Returns\",\n    x = \"Date\",\n    y = \"Daily Return\"\n  ) +\n  theme_minimal()\n\n# Create JNJ daily returns plot\np2 &lt;- ggplot(JNJ_return, aes(x = Date, y = Return)) +\n  geom_line(color = \"red\") +\n  labs(\n    title = \"JNJ Daily Returns\",\n    x = \"Date\",\n    y = \"Daily Return\"\n  ) +\n  theme_minimal()\n\n# Display side-by-side\np1 + p2\n\n\n\n\n\n\n\nCode\nmean_aapl &lt;- mean(AAPL_return$Return, na.rm = TRUE)\nsd_aapl &lt;- sd(AAPL_return$Return, na.rm = TRUE)\n\n# Plot for AAPL\np1 &lt;- ggplot(AAPL_return, aes(x = Return)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 0.01, \n                 fill = \"steelblue\", \n                 color = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean_aapl, sd = sd_aapl), \n                color = \"red\", \n                size = 1) +\n  labs(\n    title = \"AAPL Daily Returns Distribution\",\n    x = \"Daily Return\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n# Calculate mean and sd for JNJ returns\nmean_jnj &lt;- mean(JNJ_return$Return, na.rm = TRUE)\nsd_jnj &lt;- sd(JNJ_return$Return, na.rm = TRUE)\n\n# Plot for JNJ\np2 &lt;- ggplot(JNJ_return, aes(x = Return)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 0.01, \n                 fill = \"steelblue\", \n                 color = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean_jnj, sd = sd_jnj), \n                color = \"red\", \n                size = 1) +\n  labs(\n    title = \"JNJ Daily Returns Distribution\",\n    x = \"Daily Return\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n# Display both plots side-by-side using patchwork\np1 + p2\n\n\n\n\n\nWill investments on AAPL and JNJ perform similarly over time?\n\n\n2.2.2 But when it compounds…\n\n\nCode\nAAPL_compounding_return &lt;- AAPL_return %&gt;%\n  mutate(Compounding_Return = cumprod(1 + Return)) %&gt;%\n  mutate(Ticker = \"AAPL\")\n\n# Compute compounding returns for JNJ\nJNJ_compounding_return &lt;- JNJ_return %&gt;%\n  mutate(Compounding_Return = cumprod(1 + Return)) %&gt;%\n  mutate(Ticker = \"JNJ\")\n\n# Combine the two\ncombined_compounding_return &lt;- bind_rows(AAPL_compounding_return, JNJ_compounding_return)\n\n# Plot both AAPL and JNJ on the same plot\nggplot(combined_compounding_return, aes(x = Date, y = Compounding_Return, color = Ticker)) +\n  geom_line() +\n  labs(\n    title = \"AAPL and JNJ Compounding Returns Over Time\",\n    x = \"Date\",\n    y = \"Compounding Return\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n2.2.3 The Limitations of Simple Plots for Analyzing Returns\nThe daily return plot looks like noise, oscillating about 0, and we cannot make anything out of it. The histogram of returns has a mean close to 0 and resembles a normal distribution. However, when we look at the compounding return over time, it clearly goes up and is no longer messy. Why is it so hard to capture information out of the return? It’s due to market efficiency: if the returns were obviously informative, then something would be wrong, because no one in the market is foolish. If the return data showed anything other than noise, it would mean easy profit opportunities, which would quickly vanish as participants exploit them through arbitrage. Unless you engage in high-frequency trading, it’s impossible to take advantage of such fleeting signals. Thus, it is perfectly normal for the returns to look like noise, as it reflects an efficient market.\n\n\n2.2.4 Is return informative?\nBut why does the cumulative return plot appear clearer? In the long term, there are still profits to be gained, influenced by industry trends or other large-scale factors. While these factors are hidden within the daily noise, they become evident when returns accumulate. Simply using basic plots like this, however, won’t help us uncover these subtleties. There are many important patterns hidden in the return data, that are not easy to visualize. We need more complex models to make the non visible pattern visible.\n\n\nCode\naapl_stats &lt;- AAPL_return %&gt;%\n  summarize(\n    mean = mean(Return, na.rm = TRUE),\n    q25 = quantile(Return, 0.25, na.rm = TRUE),\n    q50 = quantile(Return, 0.5, na.rm = TRUE),\n    q75 = quantile(Return, 0.75, na.rm = TRUE),\n    sd = sd(Return, na.rm = TRUE)\n  )\n\n# Calculate stats for JNJ\njnj_stats &lt;- JNJ_return %&gt;%\n  summarize(\n    mean = mean(Return, na.rm = TRUE),\n    q25 = quantile(Return, 0.25, na.rm = TRUE),\n    q50 = quantile(Return, 0.5, na.rm = TRUE),\n    q75 = quantile(Return, 0.75, na.rm = TRUE),\n    sd = sd(Return, na.rm = TRUE)\n  )\n\n# Print the stats\ncat(\"AAPL Statistics:\\n\")\n\n\nAAPL Statistics:\n\n\nCode\nprint(aapl_stats)\n\n\n         mean          q25          q50        q75         sd\n1 0.001042554 -0.007387933 0.0008969619 0.01024157 0.01787848\n\n\nCode\ncat(\"\\nJNJ Statistics:\\n\")\n\n\n\nJNJ Statistics:\n\n\nCode\nprint(jnj_stats)\n\n\n          mean          q25          q50         q75         sd\n1 0.0004576925 -0.004636223 0.0004376532 0.005997842 0.01107514\n\n\nCode\n# Combine the data for plotting\ncombined_returns &lt;- rbind(\n  data.frame(Ticker = \"AAPL\", Return = AAPL_return$Return),\n  data.frame(Ticker = \"JNJ\", Return = JNJ_return$Return)\n)\n\n# Boxplot comparison\nggplot(combined_returns, aes(x = Ticker, y = Return, fill = Ticker)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of AAPL and JNJ Daily Returns\",\n    x = \"Ticker\",\n    y = \"Daily Return\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nWe can still extract some useful information from these results:\n\nAAPL return indeed has a higher mean than JNJ return.\nFrom the boxplot, we can see AAPL is more volatile than JNJ.\n(Is that essentially a good thing?)\n\n\n\nCode\nwrite.csv(sp500_stock_df, \"sp500_stock_df.csv\", row.names = FALSE)"
  },
  {
    "objectID": "data.html#overview-of-the-mean-and-volatility-of-return",
    "href": "data.html#overview-of-the-mean-and-volatility-of-return",
    "title": "2  Data",
    "section": "2.3 Overview of the mean and volatility of Return",
    "text": "2.3 Overview of the mean and volatility of Return\nBefore we enter the main part of this project, let’s first have an overview of the return data across different assets. To evaluate the data, we need 2 crucial statistics that are always the 2 most important figure of all models: mean and volatility. In financial data, it is often said “Higher the risk, higher the return”. Assets with higher mean of return always have a higher variance of return too. It means that it is very hard to find an asset, or create a portfolio that generates a consistent positive return, which is what everybody in the market is looking for.\n\n\nCode\ndaily_returns_all &lt;- function(data) {\n  # Ensure the data is sorted by Ticker and Date\n  data &lt;- data[order(data$Ticker, data$Date), ]\n  \n  # Calculate daily returns using lagged Adjusted prices for each Ticker\n  data &lt;- data %&gt;%\n    group_by(Ticker) %&gt;%                           # Group by Ticker\n    mutate(Return = (Adjusted - lag(Adjusted)) /   # Calculate return\n                      lag(Adjusted)) %&gt;%\n    ungroup()                                      # Remove grouping\n  \n  # Select only Date, Ticker, and Return columns for the output\n  result &lt;- data %&gt;% select(Date, Ticker, Return)\n  \n  return(result)\n}\n\nreturns &lt;- daily_returns_all(sp500_stock_df)\nmean_returns &lt;- aggregate(Return ~ Ticker, data = returns, FUN = mean)\nmean_returns$Ticker &lt;- reorder(mean_returns$Ticker, -mean_returns$Return)\nggplot(mean_returns, aes(x = Ticker, y = Return, fill = Ticker)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Mean Return by Asset\",\n    x = \"Asset\",\n    y = \"Mean Return\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCode\nvolatility &lt;- returns %&gt;%\n  group_by(Ticker) %&gt;%\n  summarise(Volatility = sd(Return, na.rm = TRUE))\n\n# Reorder Ticker by Volatility (high to low) for sorting in the plot\nvolatility &lt;- volatility %&gt;%\n  mutate(Ticker = reorder(Ticker, -Volatility))\n\n# Create a bar chart for volatility\nggplot(volatility, aes(x = Ticker, y = Volatility, fill = Ticker)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Volatility (Standard Deviation of Returns) by Asset\",\n    x = \"Ticker\",\n    y = \"Volatility\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n2.3.1 Traditional Strategies\nOur goal is to either find assets with higher mean, or asset with lower risk (volatility). Therefore, the 2 intuitive approaches to slect assets are:\n\nAt the same risk level, select assets with higher mean\nAt the same mean return level, select assets with lower volatility\n\n\n\nCode\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\n# Calculate mean and standard deviation (volatility) for each asset\nsummary_stats &lt;- returns %&gt;%\n  group_by(Ticker) %&gt;%\n  summarise(\n    Mean = mean(Return, na.rm = TRUE),\n    Volatility = sd(Return, na.rm = TRUE)\n  )\n\n# Create an interactive scatter plot\nfig &lt;- plot_ly(\n  data = summary_stats,\n  x = ~Volatility, \n  y = ~Mean, \n  text = ~Ticker,  # Hover text displays the asset name\n  type = 'scatter', \n  mode = 'markers',\n  marker = list(size = 10, color = 'blue')\n) %&gt;%\n  layout(\n    title = \"Mean Return vs. Volatility\",\n    xaxis = list(title = \"Volatility (Standard Deviation)\"),\n    yaxis = list(title = \"Mean Return\"),\n    hovermode = \"closest\"\n  )\n\n# Display the plot\nfig\n\n\n\n\n\n\nBy this logic, MSFT is a better choice than AAPL, NVDA is a better choice than NFLX. Because they have higher mean return and lower volatility.\nHowever, all we have done so far is analyzing past data. In reality, the mean return is one of the hardest statistics to predict. Therefore, simply looking at the statistics of past data is not sufficient enough for us to make investment decisions. In order to dig more information about an asset by its return, we need some modeling work to do."
  },
  {
    "objectID": "results.html#the-fama-french-factor-model",
    "href": "results.html#the-fama-french-factor-model",
    "title": "3  Results",
    "section": "3.1 The Fama-French Factor Model",
    "text": "3.1 The Fama-French Factor Model\nWe apply the Fama-French three-factor model to the returns of the selected assets. The model is expressed as:\n\\[\nR_i - R_f = \\alpha + \\beta_1 (R_m - R_f) + \\beta_2 \\text{SMB} + \\beta_3 \\text{HML} + \\epsilon\n\\]\nWhere: - \\(R_i\\): Return of the asset.\n\n\\(R_f\\): Risk-free rate.\n\\(R_m\\): Market return.\n\\(\\text{SMB}\\): Small-minus-Big factor, representing the size effect.\n\\(\\text{HML}\\): High-minus-Low factor, representing the value effect.\n\\(\\alpha\\): Intercept (alpha).\n\\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\): Factor loadings.\n\\(\\epsilon\\): Residual error term.\n\nInstead of focusing solely on returns, our analysis emphasizes the \\(\\alpha\\) and the factor loadings \\(\\beta_1, \\beta_2, \\beta_3\\).\nThe Fama-French three-factor model is a widely used framework in asset pricing that expands on the traditional CAPM by incorporating size and value factors to better explain stock returns. This model adds the Small-Minus-Big (SMB) factor, capturing the size effect, and the High-Minus-Low (HML) factor, representing the value effect, alongside the market risk premium. By including these additional dimensions, the Fama-French model accounts for systematic patterns in asset returns linked to company size and valuation. In this analysis, the focus lies on estimating the intercept (alpha) and factor loadings (\\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)) to understand the relationship between asset returns and these underlying factors.\nHow we apply the Fama-French Model\nWe get the factor loadings (\\(\\alpha\\) and the \\(\\beta\\)s) by linear regression. However, return data is a time series. To fit a linear model on time series, we must always be cautious because the data might evolve through time. That means, “the more data, the better” does not apply to our linear model here. Instead we pick the data from a rolling window, say, the last 30 month, and fit the linear model. We then update the linear model every month. Eventually we get a series of linear model and factor loadings. We then can evaluate the factor loadings not only by its mean, but also the distribution and consistency. We can also study how the factor loadings change through time, or the correlation between factor loadings, or across different assets.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tibble)\nlibrary(tidyr)\nsp500_stock_df &lt;- read.csv(\"sp500_stock_df.csv\")\nsp500_stock_df$Date &lt;- as.Date(sp500_stock_df$Date)\nff_df &lt;- read.csv(\"F-F_Research_Data_Factors_daily.csv\", skip = 4, header = TRUE)\nff_df &lt;- ff_df %&gt;% mutate(across(where(is.numeric), ~ . / 100))\nff_df$X &lt;- as.Date(ff_df$X, format = \"%Y%m%d\")\ncolnames(ff_df)[colnames(ff_df) == \"X\"] &lt;- \"Date\"\n\nadjusted_close &lt;- sp500_stock_df %&gt;%\n  select(Date, Adjusted, Ticker) %&gt;%  # Keep only necessary columns\n  pivot_wider(names_from = Ticker, values_from = Adjusted)\n\nreturns &lt;- adjusted_close %&gt;%\n  mutate(across(where(is.numeric), ~ c(NA, diff(.) / lag(.)[-1]))) %&gt;%\n  filter(complete.cases(.))\n\nexcess_returns &lt;- returns %&gt;%\n  left_join(ff_df, by = \"Date\") %&gt;%  # Merge by Date\n  mutate(across(\n    .cols = setdiff(names(.), c(\"Date\", \"RF\")),  # Select all columns except \"Date\" and \"RF\"\n    .fns = ~ . - RF\n  )) %&gt;%\n  select(-Mkt.RF, -SMB, -HML, -RF)\n\nff3 &lt;- ff_df %&gt;%\n  select(-RF) %&gt;% \n  filter(Date %in% excess_returns$Date)  \n\nexcess_returns_df &lt;- as.data.frame(excess_returns)\n# Convert Date column to proper Date type if needed\nexcess_returns_df$Date &lt;- as.Date(excess_returns_df$Date)\n# Set the Date column as row names\nrownames(excess_returns_df) &lt;- as.character(excess_returns_df$Date)\n# Remove the Date column\nexcess_returns_df$Date &lt;- NULL\n\nrownames(ff3) &lt;- ff3$Date\nff3$Date &lt;- NULL\n\n\n\n\nCode\nrolling_ff3_coefficients &lt;- function(returns, ff3_factors, window_size = 252, check_interval = 21) {\n  # Check that returns and ff3_factors have the same number of rows\n  if (nrow(returns) != nrow(ff3_factors)) {\n    stop(\"returns and ff3_factors must have the same number of rows.\")\n  }\n  \n  n &lt;- nrow(returns)\n  stock_names &lt;- colnames(returns)\n  factor_names &lt;- colnames(ff3_factors)\n  \n  # Initialize lists to store rolling results\n  rolling_alpha &lt;- list()\n  rolling_betas &lt;- vector(\"list\", 3) # For 3 factors\n  names(rolling_betas) &lt;- paste0(\"beta_\", 1:3)\n  \n  # Prepare empty slots for each stock\n  for (stock in stock_names) {\n    rolling_alpha[[stock]] &lt;- numeric(0)\n    for (beta_key in names(rolling_betas)) {\n      rolling_betas[[beta_key]][[stock]] &lt;- numeric(0)\n    }\n  }\n  \n  rolling_dates &lt;- character(0)\n  \n  # Iterate over rolling windows\n  for (i in seq(window_size, n, by = check_interval)) {\n    start_idx &lt;- i - window_size + 1\n    end_idx &lt;- i\n    \n    returns_window &lt;- returns[start_idx:end_idx, , drop = FALSE]\n    factors_window &lt;- ff3_factors[start_idx:end_idx, , drop = FALSE]\n    \n    # Construct the model formula: y ~ MKT_RF + SMB + HML\n    model_formula &lt;- as.formula(paste(\"y ~\", paste(factor_names, collapse = \" + \")))\n    \n    for (stock in stock_names) {\n      y &lt;- returns_window[[stock]]\n      data_for_reg &lt;- cbind(y = y, factors_window)\n      \n      # Fit the linear model\n      model &lt;- lm(model_formula, data = data_for_reg)\n      coefs &lt;- coef(model)\n      \n      # Store alpha and betas\n      rolling_alpha[[stock]] &lt;- c(rolling_alpha[[stock]], coefs[1]) # alpha (intercept)\n      for (j in 1:3) {\n        rolling_betas[[paste0(\"beta_\", j)]][[stock]] &lt;- c(rolling_betas[[paste0(\"beta_\", j)]][[stock]], coefs[j+1])\n      }\n    }\n    \n    # Record the date of the last observation in the window\n    rolling_dates &lt;- c(rolling_dates, rownames(returns)[end_idx])\n  }\n  \n  # Convert lists to data frames\n  df_alpha &lt;- as.data.frame(rolling_alpha, row.names = rolling_dates)\n  df_betas &lt;- lapply(rolling_betas, function(x) as.data.frame(x, row.names = rolling_dates))\n  \n  # Return a list with alpha and each beta DataFrame\n  return(list(\n    alpha = df_alpha,\n    beta_1 = df_betas[[\"beta_1\"]],\n    beta_2 = df_betas[[\"beta_2\"]],\n    beta_3 = df_betas[[\"beta_3\"]]\n  ))\n}\n\n\n\n\nCode\nfactor_loadings &lt;- rolling_ff3_coefficients(excess_returns_df, ff3, window_size = 630)\n\nalphas_df &lt;- factor_loadings$alpha %&gt;%\n  rownames_to_column(var = \"Date\")\n \nbeta1_df &lt;- factor_loadings$beta_1 %&gt;%\n  rownames_to_column(var = \"Date\")\n\nbeta2_df &lt;- factor_loadings$beta_2 %&gt;%\n  rownames_to_column(var = \"Date\")\n\nbeta3_df &lt;- factor_loadings$beta_3 %&gt;%\n  rownames_to_column(var = \"Date\")\n\n\n\n3.1.1 Observations on Alpha Distributions\nAlpha measures an investment’s excess return over a benchmark – the market or factor-based expectations. In models like Fama-French, it captures the portion of returns unexplained by factors such as market risk or size. In reality, alpha often reverts to zero as market inefficiencies are corrected, making it hard to find and sustain a constantly positive alpha. Investors aim for consistent, positive alpha as a source of stable excess return.\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggridges)\nalphas_long &lt;- alphas_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"Alpha\")\n\nggplot(alphas_long, aes(x = as.Date(Date), y = Alpha, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Alphas Over Time\",\n    x = \"Date\",\n    y = \"Alpha\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nalphas_long &lt;- alphas_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanAlpha = mean(Alpha)) %&gt;%\n  ungroup()\n\nggplot(alphas_long, aes(x = Alpha, y = reorder(Stock, MeanAlpha), fill = MeanAlpha)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Alpha\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Alpha Distributions by Stock\",\n    x = \"Alpha\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.000137\n\n\n\n\n\nMost alphas fluctuate around zero.\n\n3.1.1.1 Stocks with no excess return: PG, BRK.B, JNJ, KO, PFE\n\nThe alpha distributions for these low-performing stocks exhibit compact and peaked shapes, with the mean centered around zero.\nThis indicates that these stocks consistently achieve alpha values near zero, aligning closely with the efficient market hypothesis. In an efficient market, these stocks do not present persistent opportunities for abnormal returns.\n\n\n\n3.1.1.2 Stocks with stable positive excess return: AAPL, MSFT, HD, GOOGL, JPM, V\n\nThese high-performing stocks have a consistently positive alpha, the distribution is less compact, but still has a peak.\nThis indicates that these stocks consistently outperform the Fama-French Factor Model, with a constant excess return. The excess return is unexplained by Market Return, SMB and HML. We might be able to explain the excess return by introducing new factors.\nLong-term investment on these stocks will yield a positive excess return over Fama-French Factors.\n\n\n\n3.1.1.3 Stocks with very unstable positive excess return: NVDA, TSLA, NFLX, AMZN\n\nThe alpha distributions for high-performing stocks display very fat tails and lack a distinct peak, resulting in a flat and widespread shape.\nThese stocks exhibit significantly higher mean alphas, indicating periods of consistent outperformance of the efficient market. However, the flat distribution curve suggests that their alpha is highly unstable, and does not stabilize around a single value for long. Once the excess return appear, it will soon be exploited by market players, but the excess return often reappear shortly.\nThese stocks are uprising assets with great potentials and risks at the same time. The profit could be very large over a short period of time, but it is unpredictable.\n\n\n\n\n3.1.2 Observations on the Distributions of exposure to Market Risk\n\\(\\beta_1\\) in the Fama-French model represents the factor loading for market risk, measuring how sensitive an asset is to overall market movements. A \\(\\beta_1 &gt; 1\\) indicates that the asset is more volatile than the market, while \\(\\beta_1 &lt; 1\\) suggests it is less sensitive to market changes. It evaluates the portion of an asset’s returns driven by systematic market risk, making it crucial for understanding exposure to broad economic trends. Investors rely on \\(\\beta_1\\) to assess the asset’s alignment with market performance and its role in portfolio risk management.\nAlthough it often appears to be so, a high \\(\\beta_1\\) does not essentially mean a good sign. Because high volatility also brings more risk, causing significant loss when the market is going down. when the market is going upward, it is usually preferred to invest on assets with higher exposure to market risk.\n\n\nCode\nbeta1_long &lt;- beta1_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"beta1\")\n\nggplot(beta1_long, aes(x = as.Date(Date), y = beta1, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Beta1 Over Time\",\n    x = \"Date\",\n    y = \"beta1\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nbeta1_long &lt;- beta1_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanBeta1 = mean(beta1)) %&gt;%\n  ungroup()\n\nggplot(beta1_long, aes(x = beta1, y = reorder(Stock, MeanBeta1), fill = MeanBeta1)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Beta1\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Beta1 Distributions by Stock\",\n    x = \"Beta\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.0364\n\n\n\n\n\n\n3.1.2.1 Stocks with high exposure to Market Risk: NVDA, TSLA, NFLX, MSFT, JPM, AMZN, AAPL, GOOGL, etc.\nStocks that have a high \\(\\beta_1\\), reflecting significant sensitivity to market movements. These stocks often belong to technology, growth, or finance sectors, which are more volatile and influenced by broad economic trends.\n\n\n3.1.2.2 Stocks with low exposure to Market Risk: BRK.B, UNH, XOM, JNJ, KO, PG, etc.\nStocks that have a low \\(\\beta_1\\), indicating less sensitivity to market movements. These companies are often in stable industries such as consumer staples, healthcare, or energy, providing consistent performance regardless of market volatility.\nSecond Thoughts The top assets selected by \\(\\beta_1\\) are also those with high \\(alpha\\) and return. Does it mean that \\(\\beta_1\\) is a good indicator for selecting high return assets? (Will be verified in the last section)\n\n\n\n3.1.3 Observations on the Distributions of exposure to SMB and HML\nDue to the length of the project, the effects of the \\(\\beta_2\\) and \\(\\beta_3\\) will not be discussed here. The plots of the factor loadings by stocks are shown below. We can notice that the coefficient of SMB and coefficient of HML seem to be negatively correlated. This is because small-cap stocks (high SMB) often exhibit growth characteristics (low HML), while large-cap stocks (low SMB) tend to align with value characteristics (high HML).\n\n\nCode\nbeta2_long &lt;- beta2_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"beta2\")\n\nggplot(beta2_long, aes(x = as.Date(Date), y = beta2, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Beta2 Over Time\",\n    x = \"Date\",\n    y = \"beta2\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nbeta2_long &lt;- beta2_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanBeta2 = mean(beta2)) %&gt;%\n  ungroup()\n\nggplot(beta2_long, aes(x = beta2, y = reorder(Stock, MeanBeta2), fill = MeanBeta2)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Beta2\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Beta2 Distributions by Stock\",\n    x = \"Beta2\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.0471\n\n\n\n\n\n\n\nCode\nbeta3_long &lt;- beta3_df %&gt;%\n  pivot_longer(cols = -Date, names_to = \"Stock\", values_to = \"beta3\")\n\nggplot(beta3_long, aes(x = as.Date(Date), y = beta3, color = Stock)) +\n  geom_line(size = 1) +  # Use lines for trends\n  labs(\n    title = \"Beta3 Over Time\",\n    x = \"Date\",\n    y = \"beta3\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +  # Apply a clean theme\n  theme(legend.position = \"top\")\n\n\n\n\n\nCode\nbeta3_long &lt;- beta3_long %&gt;%\n  group_by(Stock) %&gt;%\n  mutate(MeanBeta3 = mean(beta3)) %&gt;%\n  ungroup()\n\nggplot(beta3_long, aes(x = beta3, y = reorder(Stock, MeanBeta3), fill = MeanBeta3)) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"Mean Beta3\", low = \"blue\", high = \"red\") +\n  labs(\n    title = \"Ridgeline Plot of Beta3 Distributions by Stock\",\n    x = \"Beta3\",\n    y = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\nPicking joint bandwidth of 0.0587\n\n\n\n\n\n\n\n3.1.4 Relationship between Factor Loadings\nAs we can see more clearly in the parallel coordinate plot, the seemingly positive relation between \\(\\alpha\\) and \\(\\beta_1\\) is not very strong. We can only say with confidence that \\(\\beta_2\\) and \\(\\beta_3\\) are indeed negatively correlated.\n\n\nCode\nalphas_median &lt;- apply(alphas_df[-1], 2, median)\nbeta1_median &lt;- apply(beta1_df[-1], 2, median)\nbeta2_median &lt;- apply(beta2_df[-1], 2, median)\nbeta3_median &lt;- apply(beta3_df[-1], 2, median)\n\nloading_df &lt;- data.frame(\n  stock = names(alphas_median),\n  alpha = alphas_median,\n  beta1 = beta1_median,\n  beta2 = beta2_median,\n  beta3 = beta3_median\n)\n\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\n# Create the parallel coordinate plot\nggparcoord(\n  data = loading_df,\n  columns = 2:5,       # Select columns for parallel coordinates (alpha, beta1, beta2, beta3)\n  groupColumn = 1,     # Use the stock column for grouping lines\n  scale = \"uniminmax\"  # Scale the data to [0, 1] for better visualization\n) +\n  labs(\n    title = \"Parallel Coordinate Plot for Stock Factor Loadings\",\n    x = \"Factors\",\n    y = \"Scaled Values\",\n    color = \"Stock\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n3.1.5 Alpha and Return\nFinally, we analyze how well \\(\\alpha\\) can reflect the return accordingly. That is, we want to find out: is the alpha generated by Fama-French model a good indicator for investment?\n\n\nCode\nlibrary(zoo)\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nCode\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\n# Set parameters\nwindow_size &lt;- 630\nupdate_interval &lt;- 21\n\nrownames_to_column &lt;- function(x, var = \"Date\") {\n  x &lt;- cbind(rowname = rownames(x), x)\n  rownames(x) &lt;- NULL\n  colnames(x)[1] &lt;- var\n  return(x)\n}\n\nexcess_returns_df &lt;- rownames_to_column(excess_returns_df)\nexcess_returns_df$Date &lt;- as.Date(excess_returns_df$Date, format = \"%Y-%m-%d\")\n\n\nexcess_zoo &lt;- zoo(excess_returns_df[,-1], order.by = excess_returns_df$Date)\n\n# Calculate rolling means with window size 630 and update interval 21\nrolling_mean &lt;- rollapply(\n  data = excess_zoo,\n  width = 630,\n  by = 21,\n  FUN = mean,\n  align = \"right\",\n  na.rm = TRUE,\n  fill = NA  # Fills incomplete windows with NA\n)\n\n# Convert the rolling mean back to a data frame\nrolling_mean_df &lt;- fortify.zoo(rolling_mean)\n\n# Rename the first column to Date\nnames(rolling_mean_df)[1] &lt;- \"Date\"\n\n# Remove rows with NA values\nrolling_mean_df &lt;- na.omit(rolling_mean_df)\n\n\nalphas_df$Date &lt;- as.Date(alphas_df$Date, format = \"%Y-%m-%d\")\n\n\nmerged_df &lt;- rolling_mean_df |&gt;\n  inner_join(alphas_df, by = \"Date\", suffix = c(\"_mean\", \"_alpha\"))\n\n\n\nlong_df &lt;- merged_df |&gt;\n  pivot_longer(\n    cols = -Date,                # Exclude the Date column\n    names_to = c(\"Stock\", \"Metric\"), # Split column names into Stock and Metric\n    names_sep = \"_\",             # Separator between Stock and Metric\n    values_to = \"Value\"          # Name of the new value column\n  ) |&gt;\n  pivot_wider(\n    names_from = \"Metric\",       # Spread Metric into separate columns\n    values_from = \"Value\"        # Populate with corresponding values\n  )\n\n\n\n\nCode\nclean_long_df &lt;- long_df |&gt;\n  drop_na(mean, alpha)\n\n#interactive plot with plotly\ninteractive_scatter &lt;- clean_long_df |&gt;\n  ggplot(aes(x = alpha, y = mean, color = Stock, text = paste(\"Stock:\", Stock))) +\n  geom_point(alpha = 0.6) +\n  labs(\n    title = \"Rolling Alpha vs Rolling Mean for Each Stock\",\n    x = \"Rolling Alpha\",\n    y = \"Rolling Mean\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n# Convert to an interactive plotly object\ninteractive_plotly &lt;- interactive_scatter |&gt;\n  ggplotly(tooltip = c(\"x\", \"y\", \"color\", \"text\"))\n\n# Display the interactive plot\ninteractive_plotly\n\n\n\n\n\n\nWe can observe that there is a strong positive linear relationship between mean return and alpha. We can design new strategies based on this finding: apply Fama-French model to the return data, and select assets with good alpha. The advantages about this strategy is that it only uses idiosyncratic return as credential, not influenced by systematic risk, like the market risk premium, SMB or HML.\nWhat does “good” mean? It can mean a consistently positive alpha, like MSFT, or an alpha with high mean but very unstable, like TSLA or NVDA, depending on the investor’s strategy.\nBy faceting by asset, we can observe that different asset have a case.\n\n\nCode\n# Create faceted scatter plots, one for each stock\nfaceted_plot &lt;- clean_long_df |&gt;\n  ggplot(aes(x = alpha, y = mean)) +\n  geom_point(color = \"steelblue\", alpha = 0.6) +\n  facet_wrap(~ Stock, scales = \"free\") +\n  labs(\n    title = \"Rolling Alpha vs Rolling Mean per Stock\",\n    x = \"Rolling Alpha\",\n    y = \"Rolling Mean\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n# Display the faceted scatter plots\nprint(faceted_plot)\n\n\n\n\n\n\n3.1.5.1 Strong Linear Relationship: DIS, PG, NFLX, TSLA\nThese assets show strong linear relationship between alpha and mean return. When using Fama-French alpha as the trading strategy, these assets would be investors’ top choice.\n\n\n3.1.5.2 Linear Relationship with Homoscedasticity Problem: BAC, GOOGL, MA\nFor these assets, the residuals of the linear model change as fitted values change. Investors should take the value into consideration. For example, when the alpha of MA is low, investors should be cautious when using alpha as the indicator, because the linear relationship between alpha and return is weak when alpha is low.\n\n\n3.1.5.3 Weak Linear Relationship: JPM, MSFT\nFor these assets, the alpha generated by the Fama-French model is not a good trading signal, because the linear relationship between alpha and return is very weak.\n\n\n\n3.1.6 Why is alpha better than betas?\nFrom the previous distribution plot, we can see that assets with high return (TSLA, NVDA, NFLX) not only have high \\(\\alpha\\), but also have high \\(\\beta_1\\), that is, they are also more exposed to the overall market performance. The following question is: can \\(\\beta_1\\) be a good indicator for trading strategies?\nIn theory, a high \\(\\beta_1\\) only means the asset is more sensitive to the market. And it would only perform well when the market is good, but performs worse when market is bad. (For example: if \\(\\beta_1=1.5\\), when the market makes 2% return, the asset will make a 3% return, but when the market makes 2% loss, the asset will also make a 3% loss) However, the plots still show that top assets tend to have high \\(\\beta_1\\). We need further analysis to verify this observation.\n\n\nCode\nbeta1_df$Date &lt;- as.Date(beta1_df$Date, format = \"%Y-%m-%d\")\n\n\nmerged_df &lt;- rolling_mean_df |&gt;\n  inner_join(beta1_df, by = \"Date\", suffix = c(\"_mean\", \"_beta1\"))\n\nlong_df &lt;- merged_df |&gt;\n  pivot_longer(\n    cols = -Date,                # Exclude the Date column\n    names_to = c(\"Stock\", \"Metric\"), # Split column names into Stock and Metric\n    names_sep = \"_\",             # Separator between Stock and Metric\n    values_to = \"Value\"          # Name of the new value column\n  ) |&gt;\n  pivot_wider(\n    names_from = \"Metric\",       # Spread Metric into separate columns\n    values_from = \"Value\"        # Populate with corresponding values\n  )\n\n\nclean_long_df &lt;- long_df |&gt;\n  drop_na(mean, beta1)\n\n#interactive plot with plotly\ninteractive_scatter &lt;- clean_long_df |&gt;\n  ggplot(aes(x = beta1, y = mean, color = Stock, text = paste(\"Stock:\", Stock))) +\n  geom_point(alpha = 0.6) +\n  labs(\n    title = \"Rolling Beta1 vs Rolling Mean for Each Stock\",\n    x = \"Rolling Beta1\",\n    y = \"Rolling Mean\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n# Convert to an interactive plotly object\ninteractive_plotly &lt;- interactive_scatter |&gt;\n  ggplotly(tooltip = c(\"x\", \"y\", \"color\", \"text\"))\n\n# Display the interactive plot\ninteractive_plotly\n\n\n\n\n\n\nFrom the scatter plot, we can see that \\(\\beta_1\\) do not affect mean return significantly. This finding supports our theory, and points out falsehood of “\\(\\beta_1\\) means high return” that the ridgeline plot has shown.\n\n3.1.6.1 Can we use beta to predict the return of specific asset?\nEven faceted by asset, it is still hard to find linear relationship between \\(\\beta_1\\) and mean return. However, we do have some new findings from the plot below, that can motivate a deeper study for a particular asset.\n\nV has a weak linear relationship\nFor assets like AAPL, GOOGL, PG, MSFT, there is an obvious non-linear relationship that can still be modeled.\n\n\n\nCode\nfaceted_plot &lt;- clean_long_df |&gt;\n  ggplot(aes(x = beta1, y = mean)) +\n  geom_point(color = \"steelblue\", alpha = 0.6) +\n  facet_wrap(~ Stock, scales = \"free\") +\n  labs(\n    title = \"Rolling Beta1 vs Rolling Mean per Stock\",\n    x = \"Rolling Beta1\",\n    y = \"Rolling Mean\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n# Display the faceted scatter plots\nprint(faceted_plot)"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "In this study, we conducted exploratory data analysis on stock data. First, we focused on analyzing returns rather than price data because returns are scale-invariant and better illustrate profitability. We found that mean returns and return variances are positively correlated; higher mean returns are associated with higher risk.\nIn addition to exploring the mean and variance of return data, we employed a factor model to extract more information about each asset based on its return data. In this project, we utilized the Fama-French three-factor model. We found that, most of the time, alpha equals zero, indicating an efficient market. We observed assets with high alpha and further differentiated between consistently high alpha assets, such as MSFT, and unstable alpha assets, such as TSLA and NVDA, which are emerging assets with significant profit potential. We also analyzed the betas to identify assets with high and low exposure to market risk. Furthermore, we found that the factor loadings for SMB and HML are negatively correlated. We can use the coefficients of SMB and HML to differentiate between small, emerging assets and large value assets.\nLastly, we examined the relationship between alpha and returns, demonstrating that high alpha indeed indicates high returns. Alpha is a highly reliable signal for investment. Additionally, we found that beta can sometimes provide misleading signals to investors, as it has a more complex relationship with returns. Therefore, caution should be exercised when using betas as investment signals."
  }
]